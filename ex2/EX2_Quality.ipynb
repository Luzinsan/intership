{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odlCapo9mjoi"
      },
      "source": [
        "# Оценка качества изображений\n",
        "\n",
        "**Постановка задачи**: Фотографии загружаемые поставщиками WB имеют разное качество: На одних может быть сложный фон, на каких-то фотографиях часть объекта не попала в кадр и.т.п. Для последующей работы с такими данными, например при использовании алгоритмов поиска по фото надо знать типs дефектов/овособенностей которые присутствуют на изображении.\n",
        "\n",
        "<img src =\"https://ml.gan4x4.ru/wb/quality/content/samples.png\" width=\"800\">\n",
        "\n",
        "\n",
        "Всего 6 типов особенностей:  \n",
        "\n",
        "* untidy,\n",
        "* angle-composition,\n",
        "* background,\n",
        "* crop,\n",
        "* text,\n",
        "* multiple-objects\n",
        "\n",
        "и один класс для изображений без дефектов\n",
        "* good-image.\n",
        "\n",
        "При этом изображение может содержать несколько видов дефектов.\n",
        "\n",
        "\n",
        "Задача:\n",
        "\n",
        "Требуется создать модель которая будет определять список дефектов для для изображения."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFhZeWyVrOxq"
      },
      "source": [
        "# Данные\n",
        "\n",
        "По [ссылке](https://ml.gan4x4.ru/wb/quality/5000/student_5000.zip) доступен архив содержащий 5000 изображений и разметку.\n",
        "\n",
        "Оригинальные изображения имели размер 900x1200 в датасете их разрешение уменьшено вдвое. Кроме изображений в архиве находиться csv файл c разметкой.\n",
        "В первой колонке имя файла с изображением (без расширения), в остальных колонках названия классов к которым относиться изображение:\n",
        "\n",
        "```\n",
        "  18715,text,multiple-objects,,\n",
        "  5259,text,background,,\n",
        "  8932,background,,,\n",
        "  ...\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZERR565as9KU"
      },
      "source": [
        "# Порядок выполнения задания\n",
        "\n",
        "Задание рекомендуется выполнять по шагам:\n",
        "\n",
        "1. Познакомьтесь с данными\n",
        "2. Выберите метрику для оценки результата\n",
        "3. Проведите анализ состояния вопроса, изучите существующие модели которые можно использовать для решения задачи\n",
        "4. Проведите EDA, опишите особенности данных и проблемы которые они могут за собой повлечь\n",
        "5. Подготовьте данные для обучения\n",
        "6. Выберите baseline модель, оцените качество её работы на данном датасете.\n",
        "7. Попробуйте улучшить значение метрики используя другую модель. Возможно обучив/дообучив ее.\n",
        "8. Оцените быстродействие выбранной модели\n",
        "9. Дайте оценку полученному результату.\n",
        "\n",
        "\n",
        "**Важно!**\n",
        "\n",
        "Блокнот должен содержать весь необходимый код для запуска финальной модели. Если для запуска требуется подгрузка весов, все ссылки длжны работать не только в вашем аккаунте но и в аккаунте преподавателя."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import libraries | Set Configs | Implementation classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --------------Libraries --------------- #\n",
        "import os, gc\n",
        "import io\n",
        "from IPython.display import clear_output\n",
        "from contextlib import redirect_stdout\n",
        "import time,random\n",
        "from typing import Optional, Tuple, Union, Callable\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "import inspect\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import accuracy_score, hamming_loss, ConfusionMatrixDisplay\n",
        "from mlcm import mlcm\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader , random_split\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from pytorch_multilabel_balanced_sampler.samplers import LeastSampledClassSampler\n",
        "\n",
        "from torchvision.transforms import v2\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torchvision.datasets import vision\n",
        "from torchsummary import summary\n",
        "\n",
        "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
        "from torchvision import tv_tensors\n",
        "\n",
        "\n",
        "import timm\n",
        "import wandb\n",
        "\n",
        "from pytorch_lightning import LightningDataModule, LightningModule, Trainer \n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "\n",
        "torch.set_float32_matmul_precision('medium')\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---Configuration class contains training configs---- #\n",
        "#                                                      #\n",
        "#                  &  model configs                    #\n",
        "# ---------------------------------------------------- #\n",
        "class Config:\n",
        "        BATCH_SIZE = 52\n",
        "        SEED = 42\n",
        "        LEARNING_RATE = 0.00878938382303729\n",
        "        EPS = 0.005663097332083254\n",
        "        WD = 0.08157514864673669\n",
        "        TYPE  = \"Multi-label Image Classification\"\n",
        "        DATA_SOURCE = \"multi-label-image-classification-dataset\"\n",
        "        MODEL_NAME = 'resnetv2_50'\n",
        "        CRITERION_ = \"Binary Cross Entropy\"\n",
        "        OPTIMIZER_ = \"AdamW\"\n",
        "        DATA_TYPE = 'image'\n",
        "        \n",
        "        EPOCHS = 100\n",
        "        NUM_WORKERS=8\n",
        "\n",
        "        PROJECT_NAME='WB intership'\n",
        "        TASK_NAME='multi-label-classification'\n",
        "\n",
        "        IMG_SIZE = (440,440)\n",
        "\n",
        "        mean = [0.5061, 0.4890, 0.4901]\n",
        "        std  = [0.4247, 0.4200, 0.4184]\n",
        "       \n",
        "        \n",
        "        def __init__(self):\n",
        "            print(\"configuration set!\")\n",
        "        \n",
        "        def check_cuda(self):\n",
        "            print(\"Scanning for CUDA\")\n",
        "            if torch.cuda.is_available():\n",
        "                print(\"GPU is available , training will be accelerated! : )\\n\")\n",
        "            else:\n",
        "                print(\"NO GPUs found : / \\n\")\n",
        "        \n",
        "        def seed_everything(self):\n",
        "            print(\"Seeding...\")\n",
        "            np.random.seed(self.SEED)\n",
        "            random.seed(self.SEED)\n",
        "            os.environ['PYTHONHASHSEED'] = str(self.SEED)\n",
        "            torch.manual_seed(self.SEED)\n",
        "            torch.cuda.manual_seed(self.SEED)\n",
        "            torch.backends.cudnn.deterministic = True\n",
        "            torch.backends.cudnn.benchmark = False\n",
        "            print(\"Seeded everything!\")\n",
        "\n",
        "Config.train_augmentations = v2.RandomApply([\n",
        "                v2.RandomHorizontalFlip(p=0.5),v2.RandomVerticalFlip(p=0.5),\n",
        "                v2.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
        "                v2.RandomPerspective(distortion_scale=0.4, p=0.5),\n",
        "                # v2.ElasticTransform(alpha=250.0, sigma=10),\n",
        "                # v2.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5.)),\n",
        "                v2.RandomPosterize(bits=2),\n",
        "            ])\n",
        "Config.train_transforms = v2.Compose([\n",
        "                Config.train_augmentations,\n",
        "                v2.Resize(Config.IMG_SIZE),\n",
        "                v2.Normalize(Config.mean, Config.std),\n",
        "])\n",
        "Config.test_transforms = v2.Compose([\n",
        "            v2.Resize(Config.IMG_SIZE),\n",
        "            v2.Normalize(Config.mean, Config.std),\n",
        "        ])\n",
        "\n",
        "CFG = Config()\n",
        "\n",
        "\n",
        "CFG.check_cuda()\n",
        "CFG.seed_everything()\n",
        "config = dict(inspect.getmembers(CFG, lambda a:not(inspect.isroutine(a))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Wildberries5000(vision.VisionDataset):\n",
        "    \"\"\"`Wildberries products <https://ml.gan4x4.ru/wb/quality/5000/student_5000.zip>`_ Dataset.\n",
        "\n",
        "    Args:\n",
        "        root (str or ``pathlib.Path``): Root directory of dataset where directory\n",
        "            ``student_5000.zip`` exists or will be saved to if download is set to True.\n",
        "        train (bool, optional): If True, creates dataset from training set, otherwise\n",
        "            creates from test set.\n",
        "        transform (callable, optional): A function/transform that takes in a PIL image\n",
        "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
        "        target_transform (callable, optional): A function/transform that takes in the\n",
        "            target and transforms it.\n",
        "        download (bool, optional): If true, downloads the dataset from the internet and\n",
        "            puts it in root directory. If dataset is already downloaded, it is not\n",
        "            downloaded again.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    url = \"https://ml.gan4x4.ru/wb/quality/5000/student_5000.zip\"\n",
        "    filename = \"student_5000.zip\"\n",
        "    data_format = '.jpg'\n",
        "    \n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        root: Union[str, pathlib.Path] = '',\n",
        "        train: bool = True,\n",
        "        split: float = 0.3,\n",
        "        transform: Optional[Callable] = None,\n",
        "        download: bool = False,\n",
        "    ) -> None:\n",
        "\n",
        "        super().__init__(root, transform=transform)\n",
        "        self.train_csv_path = pathlib.Path(self.root, \"./5000/5000.csv\")\n",
        "        self.train_dir = pathlib.Path(self.root, \"./5000/images/\")\n",
        "        self.train = train  # training set or test set\n",
        "\n",
        "        if download:\n",
        "            self.download()\n",
        "\n",
        "        if not self._check_integrity():\n",
        "            raise RuntimeError(\"Dataset not found or corrupted. You can use download=True to download it\")\n",
        "\n",
        "        index = (f'{self.train_dir}/' \\\n",
        "            + pd.read_csv(self.train_csv_path, header=None, usecols=[0], dtype=str) \\\n",
        "            + Wildberries5000.data_format)[0]\n",
        "\n",
        "        targets = pd.read_csv(self.train_csv_path, header=None, usecols = [1,2,3,4], keep_default_na=False)\n",
        "        targets = targets.apply(' '.join , axis=1)\n",
        "        targets = targets.str.strip()\n",
        "\n",
        "        self.full_dataset: pd.DataFrame = pd.DataFrame({'index': index, 'targets':targets})\n",
        "        self.full_dataset = self.full_dataset.loc[~self.full_dataset[\"index\"].isin(self.data_sanity_check())]\n",
        "\n",
        "        self.targets_list = targets.apply(lambda x: x.split())\n",
        "\n",
        "        self.mlb = MultiLabelBinarizer().fit(self.targets_list)\n",
        "        self.one_hot_targets = pd.DataFrame(self.mlb.transform(self.targets_list), columns=self.mlb.classes_)\n",
        "        self.full_dataset[self.one_hot_targets.columns] =  self.one_hot_targets\n",
        "        self.prepared = self.full_dataset.drop(['targets','good-image'], axis=1)\n",
        "\n",
        "        if split:\n",
        "            train_data, test_data = random_split(self.full_dataset, lengths=[1-split, split])\n",
        "            self.data = train_data if self.train else test_data\n",
        "        else:\n",
        "            self.data = self.full_dataset\n",
        "        \n",
        "\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "            Args:\n",
        "                idx (int): Index\n",
        "\n",
        "            Returns:\n",
        "                tuple: (image, target) where target is one-hot-encoded vector of the target classes.\n",
        "        \"\"\"\n",
        "        img = cv2.imread(self.prepared.iloc[idx,0])\n",
        "        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "        img = img / 255.\n",
        "        img = torch.Tensor(img).permute([2,1,0]) # convert to HWC\n",
        "        \n",
        "        label = self.prepared.iloc[idx, 1:].astype(np.int8).values\n",
        "        label = torch.Tensor(label)\n",
        "\n",
        "        if self.transform:                                \n",
        "            return self.transform(img), label\n",
        "        return img, label\n",
        "    \n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.prepared)\n",
        "\n",
        "    def _check_integrity(self) -> bool:\n",
        "        return os.path.isfile(pathlib.Path(self.root, Wildberries5000.filename))\n",
        "\n",
        "    def download(self) -> None:\n",
        "        if self._check_integrity():\n",
        "            print(\"Files already downloaded\")\n",
        "            return\n",
        "\n",
        "        pathlib.Path(self.root).mkdir(parents=True, exist_ok=True)\n",
        "        os.system(f'wget {Wildberries5000.url} -o {pathlib.Path(self.root, self.filename)}')\n",
        "        os.system(f'unzip {Wildberries5000.filename} -d {self.root}')\n",
        "        \n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        split = \"Train\" if self.train is True else \"Test\"\n",
        "        return f\"Split: {split}\"\n",
        "    \n",
        "    def data_sanity_check(self):\n",
        "        \"\"\"\n",
        "            this will check each image file for corrupted or missing and \n",
        "            returns index of corrupted / missing files .Doing this will\n",
        "            prevent us from running into any data errors during training phase .\n",
        "        \"\"\"\n",
        "        idx = []\n",
        "        start = time.time()\n",
        "        for i in range(len(self.full_dataset)):\n",
        "            try:#       checks for corrupeted or missing image files\n",
        "                if len(cv2.imread(self.full_dataset.iloc[i,0])) == 3:\n",
        "                    _ = 1\n",
        "            except:\n",
        "                idx.append(self.full_dataset.iloc[i,0])\n",
        "        end = time.time()\n",
        "        print(end-start)\n",
        "        _ = gc.collect()\n",
        "        print(idx)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class WBData(LightningDataModule):\n",
        "    \n",
        "    def __init__(self, dataset_class: torch.utils.data.Dataset, \n",
        "                 batch_size=CFG.BATCH_SIZE, \n",
        "                 split=0.3, # None - is inference mode\n",
        "                 train_transform=None, val_transform=None):\n",
        "        super().__init__()\n",
        "        \n",
        "        params = dict(root=pathlib.Path(os.getcwd()) / \"content\", \n",
        "                      download=True)\n",
        "        \n",
        "        if split:\n",
        "            self.train = dataset_class(train=True, transform=train_transform, split=split, **params)\n",
        "            self.train_sampler = LeastSampledClassSampler(labels=torch.tensor(self.train.one_hot_targets.values, dtype=torch.int32), \n",
        "                                                      indices=list(self.train.data.indices))\n",
        "            \n",
        "            dev = dataset_class(train=False, transform=val_transform, split=split, **params)\n",
        "            self.val, self.test = random_split(dev, lengths=[1-split, split])\n",
        "            self.val_sampler = SubsetRandomSampler(list(self.val.indices))\n",
        "            self.test_sampler = SubsetRandomSampler(list(self.test.indices))\n",
        "        else:\n",
        "            self.test = dataset_class(transform=val_transform, split=split, **params)\n",
        "            self.test_sampler = SubsetRandomSampler(list(self.test.full_dataset.index))\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.train, sampler=self.train_sampler,\n",
        "                          batch_size=self.batch_size, num_workers=CFG.NUM_WORKERS)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.val.dataset, sampler=self.val_sampler,\n",
        "                          batch_size=self.batch_size, num_workers=CFG.NUM_WORKERS)\n",
        "        \n",
        "    def test_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.test.dataset, sampler=self.test_sampler, \n",
        "                                           batch_size=1, num_workers=CFG.NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79bvnBsXim0R"
      },
      "source": [
        "## Анализ данных (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = WBData(Wildberries5000, CFG.BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 4))\n",
        "df.train.one_hot_targets.sum().plot.bar(title='Target Class Distribution')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 60% составляют изображения без особенностей\n",
        "- по 20% от всего датасета составляют изображения с фоном или с текстом\n",
        "- остальные 40% изображений имеют особенности \"композиции\", \"обрезки\", \"множества объектов\" и \"неаккуратности\" (~= по 5%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 4))\n",
        "df.train.one_hot_targets.sum(axis=1).value_counts().plot.bar(title='Distribution of Number of Labels per Image')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 88% изображений присвоена 1 метка (72% из которых принадлежат изображениям \"без особенностей\")\n",
        "- только 1% изображений имеет по 2 метки\n",
        "- 1,5% изображений имеют по 3 метки\n",
        "- 0,08% изображений присвоено 4 метки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot(imgs, row_title=None, **imshow_kwargs):\n",
        "    if not isinstance(imgs[0], list):\n",
        "        # Make a 2d grid even if there's just 1 row\n",
        "        imgs = [imgs]\n",
        "\n",
        "    num_rows = len(imgs)\n",
        "    num_cols = len(imgs[0])\n",
        "    _, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)\n",
        "    for row_idx, row in enumerate(imgs):\n",
        "        for col_idx, img in enumerate(row):\n",
        "            boxes = None\n",
        "            masks = None\n",
        "            if isinstance(img, tuple):\n",
        "                img, target = img\n",
        "                if isinstance(target, dict):\n",
        "                    boxes = target.get(\"boxes\")\n",
        "                    masks = target.get(\"masks\")\n",
        "                elif isinstance(target, tv_tensors.BoundingBoxes):\n",
        "                    boxes = target\n",
        "                else:\n",
        "                    raise ValueError(f\"Unexpected target type: {type(target)}\")\n",
        "            img = v2.functional.to_image(img)\n",
        "            if img.dtype.is_floating_point and img.min() < 0:\n",
        "                # Poor man's re-normalization for the colors to be OK-ish. This\n",
        "                # is useful for images coming out of Normalize()\n",
        "                img -= img.min()\n",
        "                img /= img.max()\n",
        "\n",
        "            img = v2.functional.to_dtype(img, torch.uint8, scale=True)\n",
        "            if boxes is not None:\n",
        "                img = draw_bounding_boxes(img, boxes, colors=\"yellow\", width=3)\n",
        "            if masks is not None:\n",
        "                img = draw_segmentation_masks(img, masks.to(torch.bool), colors=[\"green\"] * masks.shape[0], alpha=.65)\n",
        "\n",
        "            ax = axs[row_idx, col_idx]\n",
        "            ax.imshow(img.permute(1, 2, 0).numpy(), **imshow_kwargs)\n",
        "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "\n",
        "    if row_title is not None:\n",
        "        for row_idx in range(num_rows):\n",
        "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
        "\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_img_grid(samples, df, classes):\n",
        "\n",
        "    for cl in classes:\n",
        "        \n",
        "        df_cl = df.loc[df[cl] == 1].iloc[:samples]\n",
        "        for idx in range(len(df_cl)):\n",
        "            orig_img = Image.open(df_cl.iloc[idx,0])\n",
        "            \n",
        "            transform = CFG.train_augmentations\n",
        "            img = [transform(orig_img) for _ in range(samples)]\n",
        "            plot([orig_img] + img, ['\\n'.join(df_cl.iloc[idx,1].split())])\n",
        "            \n",
        "\n",
        "plot_img_grid(4, df.train.full_dataset, classes=df.train.mlb.classes_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_orig_imgs(rows, df, classes):\n",
        "    fig = plt.figure(figsize=(3.5*len(classes), 3*rows))\n",
        "\n",
        "    for cl_idx, cl in enumerate(classes):\n",
        "        df_cl = df.loc[df[cl] == 1].iloc[:rows]\n",
        "        for idx in range(len(df_cl)):\n",
        "            img = cv2.cvtColor(cv2.imread(df_cl.iloc[idx,0]), cv2.COLOR_BGR2RGB)\n",
        "            ax = fig.add_subplot(rows, len(classes), idx*len(classes)+cl_idx + 1)\n",
        "            ax.title.set_text('\\n'.join(df_cl.iloc[idx,1].split()))\n",
        "            plt.xticks([]) ; plt.yticks([]) \n",
        "            plt.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "plot_orig_imgs(4, df.train.full_dataset, classes=df.train.mlb.classes_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. angle-composition - включает фотографии позирующих моделей (в т.ч. снятых сзади), либо одежды/обуви, снятой с нетипичного ракурса (сверху, сзади, на ноге)\n",
        "2. background - фотографии, снятые не на белом фоне - т.е. с естественным фоном комнаты/улицы или с искуственным фоном\n",
        "3. crop - включает фотографии, где обрезан сам товар (на модели или без нее)\n",
        "4. good-image - модели стоят прямо перед камерой, фон белый, товар показан полностью (или в случае обуви наполовину -  1 ботинок, 1 босоножка). Среди размеченных этого класса фото были найдены фото с серым фоном, с позирующими моделями \n",
        "5. multiple-objects - фотографии с множеством объектов (одного и того же товара в разных ракурсах, одного товара в разной расцветке, на разных моделях), а также с миниатюрой/ами в углу фотографии этого товара\n",
        "6. text - фотографии, содержащие текст, логотипы, значки\n",
        "7. untidy - неопрятный товар на фотографии (непроглаженный, неаккуратно уложенный) или снятый не в студийных условиях/при плохом освещении"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-display\n",
        "from upsetplot import from_memberships, UpSet\n",
        "\n",
        "ax_dict = UpSet(from_memberships(df.train.targets_list, data=df.train.full_dataset), \n",
        "                subset_size=\"count\", show_counts=True).plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Топ 5 самый частых сочетаний меток на изображениях:\n",
        "- 3165 - good_image\n",
        "- 508 - text\n",
        "- 456 - background\n",
        "- 282 - text+background\n",
        "- 112 - untidy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calculate mean/std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ImageData(Wildberries5000):\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        img_path = self.prepared.iloc[idx, 0]\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = self.transform(image=img)['image']\n",
        "        return img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_mean_std(image_size, batch_size):\n",
        "    image_dataset = ImageData(transform=A.Compose(\n",
        "    [\n",
        "        A.Resize(height=image_size[0], width=image_size[1]),\n",
        "        A.Normalize(mean=(0, 0, 0), std=(1, 1, 1)),\n",
        "        ToTensorV2(),\n",
        "    ]), split=0.3, root=pathlib.Path(os.getcwd()) / \"content\")\n",
        "\n",
        "    image_loader = DataLoader(\n",
        "        dataset=image_dataset,\n",
        "        batch_size=batch_size,\n",
        "        sampler=LeastSampledClassSampler(labels=torch.tensor(image_dataset.one_hot_targets.values, \n",
        "                                                            dtype=torch.int32), \n",
        "                                        indices=list(image_dataset.data.indices)),\n",
        "        num_workers=CFG.NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    ####### COMPUTE MEAN / STD\n",
        "\n",
        "    # placeholders\n",
        "    psum = torch.tensor([0.0, 0.0, 0.0])\n",
        "    psum_sq = torch.tensor([0.0, 0.0, 0.0])\n",
        "\n",
        "    # loop through images\n",
        "    for inputs in tqdm(image_loader):\n",
        "        psum += inputs.sum(axis=[0, 2, 3])\n",
        "        psum_sq += (inputs**2).sum(axis=[0, 2, 3])\n",
        "\n",
        "    ####### FINAL CALCULATIONS\n",
        "\n",
        "    # pixel count\n",
        "    count = len(image_dataset) * image_size[0] * image_size[1]\n",
        "\n",
        "    # mean and std\n",
        "    total_mean = psum / count\n",
        "    total_var = (psum_sq / count) - (total_mean**2)\n",
        "    total_std = torch.sqrt(total_var)\n",
        "\n",
        "    # output\n",
        "    return total_mean, total_std\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_data(batch_size):\n",
        "    \n",
        "    print(\"mean: \" + str(CFG.mean))\n",
        "    print(\"std:  \" + str(CFG.std))\n",
        "    \n",
        "    return WBData(Wildberries5000, batch_size, \n",
        "                  train_transform=CFG.train_transforms, \n",
        "                  val_transform=CFG.test_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_mean, total_std = calc_mean_std(CFG.IMG_SIZE, CFG.BATCH_SIZE)\n",
        "print(\"mean: \" + str(total_mean))\n",
        "print(\"std:  \" + str(total_std))\n",
        "# total_mean, total_std = [0.5061, 0.4890, 0.4901], [0.4247, 0.4200, 0.4184]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = get_data(CFG.BATCH_SIZE)\n",
        "df.train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(\"================ Training phase ===============\")\n",
        "def class_distribution(df, max_test=3):\n",
        "    count = 0\n",
        "    for batch in df.train_dataloader():\n",
        "        if count > max_test: break\n",
        "        else: count+=1\n",
        "        labels = batch[1]\n",
        "        print(\"Label counts per class:\")\n",
        "        \n",
        "        good_images_count = labels.sum(axis=1).unique(return_counts=True)[1][0]\n",
        "        \n",
        "        sum_ = list(labels.sum(axis=0))\n",
        "        sum_.append(good_images_count)\n",
        "        print(sum_)\n",
        "        print(\"Difference between min and max\")\n",
        "        print(f\"{max(sum_)} - {min(sum_)}: {max(sum_) - min(sum_)}\", end=\"\\n\\n\")\n",
        "\n",
        "class_distribution(df, max_test=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLCNNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, backbone, frozen, n_classes, n_features, p_dropout):\n",
        "        super(MLCNNet,self).__init__()\n",
        "        self.model = backbone\n",
        "        if frozen:\n",
        "            for param in self.model.parameters():\n",
        "                param.requires_grad = False\n",
        "        \n",
        "        # Additional linear layer and dropout layer\n",
        "        self.classifier = nn.Sequential(nn.LazyLinear(n_features),\n",
        "                                            nn.ReLU(),\n",
        "                                            nn.Dropout(p_dropout),\n",
        "                                            nn.Linear(n_features, n_classes))\n",
        "       \n",
        "    def forward(self,x):\n",
        "        x = self.model(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LitMLCNet(LightningModule):\n",
        "    \n",
        "    def __init__(self, model, config, logger=None):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.CFG = config\n",
        "        if logger: self.custom_logger = logger\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.model(X)\n",
        "\n",
        "    def eval_loss(self, batch, batch_idx, mode):\n",
        "        x,y = batch\n",
        "        outputs = self.model(x)\n",
        "        loss = F.binary_cross_entropy_with_logits(outputs, y)\n",
        "        self.log(f\"loss/{mode}\", loss.item() / len(y), prog_bar=True, on_epoch=True)\n",
        "        return loss\n",
        "    \n",
        "    def eval_accuracy(self, batch, batch_idx, mode):\n",
        "        X,y = batch\n",
        "        Out  = self(X)\n",
        "        y_hat = torch.sigmoid(Out).round()\n",
        "        accuracy = accuracy_score(y.detach().cpu(), y_hat.detach().cpu())\n",
        "        self.log(f\"accuracy/{mode}\", accuracy, prog_bar=True, on_epoch=True)\n",
        "        return accuracy\n",
        "\n",
        "    def hamming_loss(self, batch, batch_idx, mode):\n",
        "        X,y = batch\n",
        "        Out  = self(X)\n",
        "        y_hat = torch.sigmoid(Out).round()\n",
        "        loss = hamming_loss(y.detach().cpu(), y_hat.detach().cpu())\n",
        "        self.log(f\"hamming_loss/{mode}\", loss, prog_bar=True, on_epoch=True)\n",
        "        return loss\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        self.eval_accuracy(batch, batch_idx, 'train')\n",
        "        self.hamming_loss(batch, batch_idx, 'train')\n",
        "        return self.eval_loss(batch, batch_idx, 'train')\n",
        "    \n",
        "    def validation_step(self,batch,batch_idx):\n",
        "        self.eval_loss(batch, batch_idx, 'val')\n",
        "        self.eval_accuracy(batch, batch_idx, 'val')\n",
        "        self.hamming_loss(batch, batch_idx, 'val')\n",
        "    \n",
        "    def test_step(self, batch, batch_idx):\n",
        "        self.eval_accuracy(batch, batch_idx, 'test')\n",
        "        self.hamming_loss(batch, batch_idx, 'test')\n",
        "\n",
        "    def confusion_matrix(self, trainer, df):\n",
        "       \n",
        "        preds_labels = trainer.predict(self, df.test_dataloader())\n",
        "\n",
        "        preds,labels =[],[]\n",
        "        for item in preds_labels:\n",
        "            preds.append(torch.round(torch.sigmoid(item[0][0])).detach().cpu().numpy().tolist())\n",
        "            labels.append(item[1][0].detach().cpu().numpy().tolist())\n",
        "\n",
        "        # Матрица ошибок с помощью mlcm:\n",
        "        conf_mat, normal_conf_mat = mlcm.cm(labels, preds)\n",
        "\n",
        "        fig = plt.figure(figsize=(20, 20))\n",
        "\n",
        "        ax1 = fig.add_subplot(2,2,1)\n",
        "        matrix_classes = np.delete(df.train.mlb.classes_, np.where(df.train.mlb.classes_ == 'good-image'))\n",
        "        matrix_classes = np.append(matrix_classes, 'good_image')\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=conf_mat, display_labels=matrix_classes)\n",
        "        disp.plot(cmap=plt.cm.Blues, xticks_rotation=90, ax=ax1)\n",
        "        plt.title('Raw confusion Matrix:')\n",
        "\n",
        "        ax2 = fig.add_subplot(2,2,2)\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=normal_conf_mat, display_labels=matrix_classes)\n",
        "        disp.plot(cmap=plt.cm.Blues, xticks_rotation=90, ax=ax2)\n",
        "        plt.title('Normalized confusion Matrix (%)')\n",
        "        plt.show()\n",
        "\n",
        "        self.statistics(conf_mat, matrix_classes)\n",
        "\n",
        "\n",
        "    def statistics(self, conf_mat, matrix_classes):\n",
        "        f = io.StringIO()\n",
        "        with redirect_stdout(f):\n",
        "            bins_conf_matrix = mlcm.stats(conf_mat, print_binary_mat=False)\n",
        "        out = f.getvalue()\n",
        "\n",
        "        stats = [row.split() for row in out.split('\\n') if len(row) > 0]\n",
        "        stats[-3:] = [['-'.join(row[:2])] + row[2:] for row in stats[-3:]]\n",
        "        stats = np.array(stats)\n",
        "        index = np.concatenate([matrix_classes, stats[-3:,0]])\n",
        "\n",
        "        stats = pd.DataFrame(stats[1:, 1:], columns=stats[0][1:], index=index)\n",
        "        stats.style.set_properties(subset=['precision'], **{'width': '25px'})\n",
        "        if hasattr(self, 'custom_logger'): \n",
        "            self.custom_logger.log_table(\"statistics_table\", dataframe=stats)\n",
        "        print(stats)\n",
        "        self.confusion_matrix_per_classes(bins_conf_matrix, matrix_classes)\n",
        "\n",
        "    def confusion_matrix_per_classes(self, bins_conf_matrix, matrix_classes):\n",
        "        fig = plt.figure(figsize=(20, 20))\n",
        "        for idx, matrix in enumerate(bins_conf_matrix, 1):\n",
        "            ax = fig.add_subplot(4,4,idx)\n",
        "            ax.title.set_text(matrix_classes[idx-1])\n",
        "            confusion_matrix = ConfusionMatrixDisplay(confusion_matrix=matrix)\n",
        "            confusion_matrix.plot(ax=ax, cmap=plt.cm.Blues)\n",
        "        plt.show()\n",
        "\n",
        "    def predict_step(self, batch, batch_idx=0,dataloader_idx=0):\n",
        "        X, y = batch\n",
        "        Out = self(X)\n",
        "        return Out, y\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        optim = torch.optim.AdamW(self.parameters(), lr=self.CFG.LEARNING_RATE, \n",
        "                                  eps=self.CFG.EPS, weight_decay=self.CFG.WD)\n",
        "        return optim\n",
        "    \n",
        "timm.list_models(\"resnet*\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(config=None, pl_Model=None):\n",
        "    assert pl_Model\n",
        "    with wandb.init(config=config) as run:\n",
        "        if not config: config = wandb.config\n",
        "        logger = WandbLogger(project=CFG.PROJECT_NAME)\n",
        "        pl_Model.custom_logger = logger\n",
        "        trainer = Trainer(\n",
        "            # max_epochs=3,\n",
        "            accelerator='gpu',\n",
        "            devices=1,\n",
        "            callbacks=[EarlyStopping(monitor=\"loss/train_epoch\", mode=\"min\", patience=10)],\n",
        "            log_every_n_steps=5,\n",
        "            logger=logger\n",
        "        )\n",
        "        df = get_data(CFG.BATCH_SIZE)\n",
        "        trainer.fit(model=pl_Model,\n",
        "                    train_dataloaders=df.train_dataloader(),\n",
        "                    val_dataloaders=df.val_dataloader())\n",
        "        os.makedirs(\"./weights/\", exist_ok=True)\n",
        "        torch.save(pl_Model.model.state_dict(), 'weights/model.pth')\n",
        "        artifact = wandb.Artifact('model', type='model')\n",
        "        artifact.add_file('weights/model.pth')\n",
        "        run.log_artifact(artifact)\n",
        "\n",
        "        trainer.test(pl_Model, df.test_dataloader())\n",
        "        pl_Model.confusion_matrix(trainer, df)\n",
        "    return trainer, pl_Model, logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "timm.list_models('*mobilenet*', pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CFG.MODEL_NAME = 'resnetv2_50'\n",
        "backbone = timm.create_model(CFG.MODEL_NAME, pretrained=True, num_classes=0) \n",
        "model = MLCNNet(backbone, True, 6, \n",
        "                512, 0.5)\n",
        "pl_Model = LitMLCNet(model, CFG)\n",
        "pl_Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pl_Model.to('cuda')\n",
        "summary(pl_Model, (3, *CFG.IMG_SIZE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer, pl_Model, logger = train(CFG, pl_Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##############---OPTIMIZATION-HP---##############\n",
        "# !export WANDB_NOTEBOOK_NAME='EX2_Quality.ipynb'\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'name': 'optimize',\n",
        "    'project':CFG.PROJECT_NAME,\n",
        "    'metric': {\n",
        "        'name': 'accuracy/val',\n",
        "        'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'epochs': {\n",
        "            'value': 5\n",
        "            },\n",
        "        'learning_rate': {\n",
        "            # a flat distribution between 0 and 0.1\n",
        "            'distribution': 'uniform',\n",
        "            'min': 0,\n",
        "            'max': 0.01\n",
        "            },\n",
        "        'eps': {\n",
        "            # a flat distribution between 0 and 0.1\n",
        "            'distribution': 'uniform',\n",
        "            'min': 0,\n",
        "            'max': 1e-2\n",
        "            },\n",
        "        'wd': {\n",
        "            # a flat distribution between 0 and 0.1\n",
        "            'distribution': 'uniform',\n",
        "            'min': 0,\n",
        "            'max': 0.1\n",
        "            },\n",
        "        'batch_size': {\n",
        "            # integers between 32 and 256\n",
        "            # # with evenly-distributed logarithms \n",
        "            'distribution': 'q_log_uniform_values',\n",
        "            'q': 8,\n",
        "            'min': 32,\n",
        "            'max': 256,\n",
        "            },\n",
        "        'image_size': {\n",
        "            # integers between 32 and 256\n",
        "            # # with evenly-distributed logarithms \n",
        "            'distribution': 'q_log_uniform_values',\n",
        "            'q': 8,\n",
        "            'min': 32,\n",
        "            'max': 512,\n",
        "            },\n",
        "        'fc_layer_size': {\n",
        "            'values': [128, 256, 512]\n",
        "            },\n",
        "        'dropout': {\n",
        "            'values': [0.3, 0.4, 0.5, 0.6]\n",
        "            },\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config)\n",
        "wandb.agent(sweep_id, function=train, count=150)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZ06649Gi6lL"
      },
      "source": [
        "## Метрики"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcxdaRH0jT8J"
      },
      "source": [
        "## Решение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryhpxGvWjeG3"
      },
      "outputs": [],
      "source": [
        "CFG.MODEL_NAME = \"mobilenetv2_050.lamb_in1k\"\n",
        "backbone = timm.create_model(CFG.MODEL_NAME, pretrained=True, num_classes=0) \n",
        "model = MLCNNet(backbone, False, 6, \n",
        "                512, 0.5)\n",
        "pl_Model = LitMLCNet(model, CFG)\n",
        "pl_Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pl_Model.to('cuda')\n",
        "summary(pl_Model, (3, *CFG.IMG_SIZE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer, pl_Model, logger = train(CFG, pl_Model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RevAHksWjWFd"
      },
      "source": [
        "## Оценка результата"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_image(df, idx):\n",
        "    img = cv2.imread(df.iloc[idx,0])\n",
        "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "    img_prepared = img / 255.\n",
        "    img_prepared = torch.Tensor(img_prepared).permute([2,1,0]) # convert to HWC\n",
        "\n",
        "    label = df.iloc[idx, 1:].astype(np.int8).values\n",
        "    label = torch.Tensor(label)\n",
        "    transform = CFG.test_transforms\n",
        "    X = transform(img_prepared)\n",
        "    pred = torch.sigmoid(pl_Model(X.unsqueeze(0))[0].detach().cpu()).round().numpy().tolist()\n",
        "    return img, label.numpy().tolist(), pred\n",
        "\n",
        "def print_test(classes, df_test):\n",
        "    len_classes = len(classes)\n",
        "    fig = plt.figure(figsize=(len_classes*3, len_classes*4))\n",
        "    fig.suptitle('true:\\npredicted:', fontsize=16)\n",
        "    for cl_idx, cl in enumerate(classes):\n",
        "        df_cl = df_test.loc[df_test.loc[:,cl] == 1]\n",
        "        for idx in range(len_classes):\n",
        "            img, label, pred = test_image(df_cl, idx)\n",
        "            \n",
        "            ax = fig.add_subplot(len_classes, len_classes, idx*len_classes+cl_idx + 1)\n",
        "            true_labels, predicted_labels = classes[list(map(bool,label))], classes[list(map(bool,pred))]\n",
        "            true_labels, predicted_labels = list(true_labels) if len(true_labels) else ['good-image'], \\\n",
        "                                            list(predicted_labels) if len(predicted_labels) else ['good-image']\n",
        "            if true_labels == predicted_labels:\n",
        "                img_title = '\\n'.join(true_labels)\n",
        "                plt.setp(ax.title, color='g')\n",
        "            else:\n",
        "                img_title = '\\n'.join(true_labels) \\\n",
        "                            + \"\\n---\\n\" \\\n",
        "                            + '\\n'.join(predicted_labels)\n",
        "                plt.setp(ax.title, color='r')\n",
        "            ax.title.set_text(img_title)\n",
        "            plt.xticks([]) ; plt.yticks([]) \n",
        "            plt.imshow(img)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "with wandb.init() as run:\n",
        "    model_v = 'model:v15'\n",
        "    artifact = run.use_artifact(f'luzinsan/intership-ex2/{model_v}', type='model')\n",
        "    artifact_dir = artifact.download()\n",
        "\n",
        "    backbone = timm.create_model(\"resnetv2_50\", pretrained=True, num_classes=0)\n",
        "    Model = MLCNNet(backbone, False, 6, 512, 0.5)\n",
        "    logger = WandbLogger(project=CFG.PROJECT_NAME)\n",
        "    pl_Model = LitMLCNet(Model, CFG, logger)\n",
        "    pl_Model.model.load_state_dict(torch.load(f'artifacts/{model_v}/model.pth'))\n",
        "    pl_Model.eval()\n",
        "\n",
        "    trainer = Trainer(\n",
        "                accelerator='gpu',\n",
        "                devices=1,\n",
        "                log_every_n_steps=5,\n",
        "                logger=logger)\n",
        "    df = get_data(CFG.BATCH_SIZE)\n",
        "    clear_output()\n",
        "    \n",
        "    print(\"Test Size: \", len(df.test))\n",
        "    trainer.test(pl_Model, df.test_dataloader())\n",
        "    pl_Model.confusion_matrix(trainer, df)\n",
        "    pl_Model.eval()\n",
        "    classes = np.delete(df.train.mlb.classes_, np.where(df.train.mlb.classes_ == 'good-image'))\n",
        "    df_test = df.test.dataset.prepared.iloc[df.test.indices]\n",
        "    print_test(classes, df_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXacMHM3hG90"
      },
      "source": [
        "## Вывод"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwmp5GtDiCWV"
      },
      "source": [
        "# Тестовый блок для проверки\n",
        "\n",
        "Поместите сюда весь необходимый код для тестирования вашей модели на новых данных. Убедитесь что\n",
        "\n",
        "- Импортируются все библиотеки и классы\n",
        "- Подгружабтся веса с внешних ресурсов\n",
        "- Происходит рассчет метрик\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##################---LIBRARIES---##################\n",
        "from torchvision.datasets import vision\n",
        "from typing import Union, Optional, Callable, Tuple\n",
        "import pathlib\n",
        "import torch\n",
        "from torch import nn\n",
        "from lightning import LightningDataModule\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import accuracy_score, hamming_loss, ConfusionMatrixDisplay\n",
        "from mlcm import mlcm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import lightning as pl\n",
        "from torchvision.transforms import v2\n",
        "import cv2\n",
        "import os, gc, io\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "from contextlib import redirect_stdout\n",
        "\n",
        "import wandb\n",
        "import timm, time\n",
        "\n",
        "wandb.login(anonymous='allow')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##################---DATA---##################\n",
        "class Wildberries5000(vision.VisionDataset):\n",
        "    \"\"\"`Wildberries products <https://ml.gan4x4.ru/wb/quality/5000/student_5000.zip>`_ Dataset.\n",
        "\n",
        "    Args:\n",
        "        root (str or ``pathlib.Path``): Root directory of dataset where directory\n",
        "            ``student_5000.zip`` exists or will be saved to if download is set to True.\n",
        "        transform (callable, optional): A function/transform that takes in a PIL image\n",
        "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
        "        download (bool, optional): If true, downloads the dataset from the internet and\n",
        "            puts it in root directory. If dataset is already downloaded, it is not\n",
        "            downloaded again.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    url = \"https://ml.gan4x4.ru/wb/quality/5000/student_5000.zip\"\n",
        "    filename = \"student_5000.zip\"\n",
        "    data_format = '.jpg'\n",
        "    \n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        root: Union[str, pathlib.Path] = '',\n",
        "        transform: Optional[Callable] = None,\n",
        "        download: bool = False,\n",
        "    ) -> None:\n",
        "\n",
        "        super().__init__(root, transform=transform)\n",
        "        self.csv_path = pathlib.Path(self.root, \"./5000/5000.csv\")\n",
        "        self.dir = pathlib.Path(self.root, \"./5000/images/\")\n",
        "        \n",
        "\n",
        "        if download:\n",
        "            self.download()\n",
        "\n",
        "        if not self._check_integrity():\n",
        "            raise RuntimeError(\"Dataset not found or corrupted. You can use download=True to download it\")\n",
        "\n",
        "        index = (f'{self.dir}/' \\\n",
        "            + pd.read_csv(self.csv_path, header=None, usecols=[0], dtype=str) \\\n",
        "            + Wildberries5000.data_format)[0]\n",
        "\n",
        "        targets = pd.read_csv(self.csv_path, header=None, usecols = [1,2,3,4], keep_default_na=False)\n",
        "        targets = targets.apply(' '.join , axis=1)\n",
        "        targets = targets.str.strip()\n",
        "\n",
        "        self.full_dataset: pd.DataFrame = pd.DataFrame({'index': index, 'targets':targets})\n",
        "        self.full_dataset = self.full_dataset.loc[~self.full_dataset[\"index\"].isin(self.data_sanity_check())]\n",
        "\n",
        "        self.targets_list = targets.apply(lambda x: x.split())\n",
        "\n",
        "        self.mlb = MultiLabelBinarizer().fit(self.targets_list)\n",
        "        self.one_hot_targets = pd.DataFrame(self.mlb.transform(self.targets_list), columns=self.mlb.classes_)\n",
        "        self.full_dataset[self.one_hot_targets.columns] =  self.one_hot_targets\n",
        "        self.prepared = self.full_dataset.drop(['targets','good-image'], axis=1)\n",
        "\n",
        "        self.data = self.full_dataset\n",
        "        \n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        img = cv2.imread(self.prepared.iloc[idx,0])\n",
        "        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "        img = img / 255.\n",
        "        img = torch.Tensor(img).permute([2,1,0]) # convert to HWC\n",
        "        \n",
        "        label = self.prepared.iloc[idx, 1:].astype(np.int8).values\n",
        "        label = torch.Tensor(label)\n",
        "\n",
        "        if self.transform:                                \n",
        "            return self.transform(img), label\n",
        "        return img, label\n",
        "    \n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.prepared)\n",
        "\n",
        "    def _check_integrity(self) -> bool:\n",
        "        return os.path.isfile(pathlib.Path(self.root, Wildberries5000.filename))\n",
        "\n",
        "    def download(self) -> None:\n",
        "        if self._check_integrity():\n",
        "            print(\"Files already downloaded\")\n",
        "            return\n",
        "\n",
        "        pathlib.Path(self.root).mkdir(parents=True, exist_ok=True)\n",
        "        os.system(f'wget {Wildberries5000.url} -o {pathlib.Path(self.root, self.filename)}')\n",
        "        os.system(f'unzip {Wildberries5000.filename} -d {self.root}')\n",
        "        \n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        split = \"Test\"\n",
        "        return f\"Split: {split}\"\n",
        "    \n",
        "    def data_sanity_check(self):\n",
        "        \"\"\"\n",
        "            this will check each image file for corrupted or missing and \n",
        "            returns index of corrupted / missing files .Doing this will\n",
        "            prevent us from running into any data errors during training phase .\n",
        "        \"\"\"\n",
        "        idx = []\n",
        "        start = time.time()\n",
        "        for i in range(len(self.full_dataset)):\n",
        "            try:#       checks for corrupeted or missing image files\n",
        "                if len(cv2.imread(self.full_dataset.iloc[i,0])) == 3:\n",
        "                    _ = 1\n",
        "            except:\n",
        "                idx.append(self.full_dataset.iloc[i,0])\n",
        "        end = time.time()\n",
        "        print(end-start)\n",
        "        _ = gc.collect()\n",
        "        print(idx)\n",
        "        return idx\n",
        "    \n",
        "class WBData(LightningDataModule):\n",
        "    \n",
        "    def __init__(self, dataset_class: torch.utils.data.Dataset, \n",
        "                 batch_size=52, \n",
        "                 val_transform=None):\n",
        "        super().__init__()\n",
        "        \n",
        "        params = dict(root=pathlib.Path(os.getcwd()) / \"content\", \n",
        "                      download=True)\n",
        "        \n",
        "        self.test = dataset_class(transform=val_transform, **params)\n",
        "        self.test_sampler = SubsetRandomSampler(list(self.test.full_dataset.index))\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        \n",
        "    def test_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.test, sampler=self.test_sampler, \n",
        "                                           batch_size=1, num_workers=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##################---MODEL---##################\n",
        "class MLCNNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, backbone, frozen, n_classes, n_features, p_dropout):\n",
        "        super(MLCNNet,self).__init__()\n",
        "        self.model = backbone\n",
        "        self.classifier = nn.Sequential(nn.LazyLinear(n_features),\n",
        "                                            nn.ReLU(),\n",
        "                                            nn.Dropout(p_dropout),\n",
        "                                            nn.Linear(n_features, n_classes))\n",
        "       \n",
        "    def forward(self,x):\n",
        "        x = self.model(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "    \n",
        "\n",
        "class LitMLCNet(pl.LightningModule):\n",
        "    \n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.model(X)\n",
        "    \n",
        "    def eval_accuracy(self, batch, batch_idx, mode):\n",
        "        X,y = batch\n",
        "        Out  = self(X)\n",
        "        y_hat = torch.sigmoid(Out).round()\n",
        "        accuracy = accuracy_score(y.detach().cpu(), y_hat.detach().cpu())\n",
        "        self.log(f\"accuracy/{mode}\", accuracy, prog_bar=True, on_epoch=True)\n",
        "        return accuracy\n",
        "\n",
        "    def hamming_loss(self, batch, batch_idx, mode):\n",
        "        X,y = batch\n",
        "        Out  = self(X)\n",
        "        y_hat = torch.sigmoid(Out).round()\n",
        "        loss = hamming_loss(y.detach().cpu(), y_hat.detach().cpu())\n",
        "        self.log(f\"hamming_loss/{mode}\", loss, prog_bar=True, on_epoch=True)\n",
        "        return loss\n",
        "    \n",
        "    def test_step(self, batch, batch_idx):\n",
        "        self.eval_accuracy(batch, batch_idx, 'test')\n",
        "        self.hamming_loss(batch, batch_idx, 'test')\n",
        "\n",
        "    def confusion_matrix(self, trainer, df):\n",
        "        preds_labels = trainer.predict(self, df.test_dataloader())\n",
        "\n",
        "        preds,labels =[],[]\n",
        "        for item in preds_labels:\n",
        "            preds.append(torch.round(torch.sigmoid(item[0][0])).detach().cpu().numpy().tolist())\n",
        "            labels.append(item[1][0].detach().cpu().numpy().tolist())\n",
        "\n",
        "        # Матрица ошибок с помощью mlcm:\n",
        "        conf_mat, normal_conf_mat = mlcm.cm(labels, preds)\n",
        "\n",
        "        fig = plt.figure(figsize=(20, 20))\n",
        "\n",
        "        ax1 = fig.add_subplot(2,2,1)\n",
        "        matrix_classes = np.delete(df.test.mlb.classes_, np.where(df.test.mlb.classes_ == 'good-image'))\n",
        "        matrix_classes = np.append(matrix_classes, 'good_image')\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=conf_mat, display_labels=matrix_classes)\n",
        "        disp.plot(cmap=plt.cm.Blues, xticks_rotation=90, ax=ax1)\n",
        "        plt.title('Raw confusion Matrix:')\n",
        "\n",
        "        ax2 = fig.add_subplot(2,2,2)\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=normal_conf_mat, display_labels=matrix_classes)\n",
        "        disp.plot(cmap=plt.cm.Blues, xticks_rotation=90, ax=ax2)\n",
        "        plt.title('Normalized confusion Matrix (%)')\n",
        "        plt.show()\n",
        "\n",
        "        self.statistics(conf_mat, matrix_classes)\n",
        "\n",
        "\n",
        "    def statistics(self, conf_mat, matrix_classes):\n",
        "        f = io.StringIO()\n",
        "        with redirect_stdout(f):\n",
        "            bins_conf_matrix = mlcm.stats(conf_mat, print_binary_mat=False)\n",
        "        out = f.getvalue()\n",
        "\n",
        "        stats = [row.split() for row in out.split('\\n') if len(row) > 0]\n",
        "        stats[-3:] = [['-'.join(row[:2])] + row[2:] for row in stats[-3:]]\n",
        "        stats = np.array(stats)\n",
        "        index = np.concatenate([matrix_classes, stats[-3:,0]])\n",
        "\n",
        "        stats = pd.DataFrame(stats[1:, 1:], columns=stats[0][1:], index=index)\n",
        "        stats.style.set_properties(subset=['precision'], **{'width': '25px'})\n",
        "        if hasattr(self, 'custom_logger'): \n",
        "            self.custom_logger.log_table(\"statistics_table\", dataframe=stats)\n",
        "        print(stats)\n",
        "        self.confusion_matrix_per_classes(bins_conf_matrix, matrix_classes)\n",
        "\n",
        "    def confusion_matrix_per_classes(self, bins_conf_matrix, matrix_classes):\n",
        "        fig = plt.figure(figsize=(20, 20))\n",
        "        for idx, matrix in enumerate(bins_conf_matrix, 1):\n",
        "            ax = fig.add_subplot(4,4,idx)\n",
        "            ax.title.set_text(matrix_classes[idx-1])\n",
        "            confusion_matrix = ConfusionMatrixDisplay(confusion_matrix=matrix)\n",
        "            confusion_matrix.plot(ax=ax, cmap=plt.cm.Blues)\n",
        "        plt.show()\n",
        "\n",
        "    def predict_step(self, batch):\n",
        "        X, y = batch\n",
        "        Out = self(X)\n",
        "        return Out, y\n",
        "    \n",
        "\n",
        "def test_image(df, idx, test_transforms):\n",
        "    img = cv2.imread(df.iloc[idx,0])\n",
        "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "    img_prepared = img / 255.\n",
        "    img_prepared = torch.Tensor(img_prepared).permute([2,1,0]) # convert to HWC\n",
        "\n",
        "    label = df.iloc[idx, 1:].astype(np.int8).values\n",
        "    label = torch.Tensor(label)\n",
        "    transform = test_transforms\n",
        "    X = transform(img_prepared)\n",
        "    pred = torch.sigmoid(pl_Model(X.unsqueeze(0))[0].detach().cpu()).round().numpy().tolist()\n",
        "    return img, label.numpy().tolist(), pred\n",
        "\n",
        "\n",
        "def print_test(classes, df_test, test_transforms):\n",
        "    len_classes = len(classes)\n",
        "    fig = plt.figure(figsize=(len_classes*3, len_classes*4))\n",
        "    fig.suptitle('true:\\npredicted:', fontsize=16)\n",
        "    for cl_idx, cl in enumerate(classes):\n",
        "        df_cl = df_test.loc[df_test.loc[:,cl] == 1]\n",
        "        for idx in range(len_classes):\n",
        "            img, label, pred = test_image(df_cl, idx, test_transforms)\n",
        "            \n",
        "            ax = fig.add_subplot(len_classes, len_classes, idx*len_classes+cl_idx + 1)\n",
        "            true_labels, predicted_labels = classes[list(map(bool,label))], classes[list(map(bool,pred))]\n",
        "            true_labels, predicted_labels = list(true_labels) if len(true_labels) else ['good-image'], \\\n",
        "                                            list(predicted_labels) if len(predicted_labels) else ['good-image']\n",
        "            if true_labels == predicted_labels:\n",
        "                img_title = '\\n'.join(true_labels)\n",
        "                plt.setp(ax.title, color='g')\n",
        "            else:\n",
        "                img_title = '\\n'.join(true_labels) \\\n",
        "                            + \"\\n---\\n\" \\\n",
        "                            + '\\n'.join(predicted_labels)\n",
        "                plt.setp(ax.title, color='r')\n",
        "            ax.title.set_text(img_title)\n",
        "            plt.xticks([]) ; plt.yticks([]) \n",
        "            plt.imshow(img)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNSZBBobii7H"
      },
      "outputs": [],
      "source": [
        "with wandb.init() as run:\n",
        "    # Загрузка тестовых данных\n",
        "    Wildberries5000.url = \"https://ml.gan4x4.ru/wb/quality/5000/student_5000.zip\"\n",
        "    Wildberries5000.filename = \"student_5000.zip\"\n",
        "    Wildberries5000.data_format = '.jpg'\n",
        "\n",
        "    test_transforms = v2.Compose([\n",
        "            v2.Resize((224,224)),\n",
        "            v2.Normalize([0.5061, 0.4890, 0.4901], \n",
        "                         [0.4247, 0.4200, 0.4184]),\n",
        "        ])\n",
        "    df = WBData(Wildberries5000, 32, \n",
        "                val_transform=test_transforms)\n",
        "    \n",
        "    # Загрузка весов модели\n",
        "    model_v = 'model:v15'\n",
        "    artifact = run.use_artifact(f'luzinsan/intership-ex2/{model_v}', type='model')\n",
        "    artifact_dir = artifact.download()\n",
        "\n",
        "    # Инициализация модели\n",
        "    backbone = timm.create_model(\"resnetv2_50\", num_classes=0)\n",
        "    Model = MLCNNet(backbone, False, 6, 512, 0.5)\n",
        "    pl_Model = LitMLCNet(Model)\n",
        "    pl_Model.model.load_state_dict(torch.load(f'artifacts/{model_v}/model.pth'))\n",
        "    pl_Model.eval()\n",
        "    trainer = pl.Trainer()\n",
        "    clear_output()\n",
        "\n",
        "    # Вычисление accuracy и hamming_loss на тестовом датасете\n",
        "    print(\"Test Size: \", len(df.test))\n",
        "    trainer.test(pl_Model, df.test_dataloader())\n",
        "    # матрица ошибок\n",
        "    pl_Model.confusion_matrix(trainer, df)\n",
        "\n",
        "    # посмотрим с картинками\n",
        "    classes = np.delete(df.test.mlb.classes_, np.where(df.test.mlb.classes_ == 'good-image'))\n",
        "    print_test(classes, df.test.prepared, test_transforms)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
